# 设计模式

- 高内聚，低耦合；

## UML


## 7个基本原则
1. **单一职责**  Single Responsibility Principle SRP   一个类只有一个引起它变化的原因；
2. **开闭原则**  Open-Closed Principle OCP   软件实体应尽量在不修改原有代码的情况下进行扩展；需要通过抽象化设计，为系统设计一个相对稳定的抽象层，而将不同的实现行为移至具体的实现层中完成；
3. **里氏代换原则**  Liskov Substitution Principle LSP  所有引用基类的地方必须能透明地使用其子类的对象，即软件中将一个基类对象替换成它的子类对象 程序将不会产生任何错误和异常；
4. **依赖倒转原则**  Dependency Inversion Principle DIP  要针对接口编程，而不是针对实现编程；
   - 在实现依赖倒转原则时，需要针对抽象层编程，而将具体类的对象通过依赖注入Dependency Injection的方式注入到其他对象中
   - 常用的注入方式有三种：（在定义时使用抽象类型，在运行时传入具体类型）
     - 构造注入  通过构造函数传入
     - 设值注入  通过Setter方法传入
     - 接口注入  通过实现在接口中声明的业务方法来传入具体类的对象
   - 依赖于抽象类，程序运行时，根据配置文件，确定具体类型；
5. **接口隔离原则**  Interface Segregation Principle ISP  使用多个专门的接口，而不使用单一的总接口，即客户端不应该依赖那些它不需要的接口
6. **合成复用原则** Composite Reuse Principle  CRP  尽量使用对象组合，而不是继承来达到复用的目的；一个新的对象通过关联关系（组合关系和聚合关系[Has-A关系]）来使用一些已有的对象（黑箱复用），使之成为新对象的一部分；
   - 组合关系
   - 聚合关系
7. **迪米特法则/最少知识原则**  Law of Demeter  LoD  一个实体应当尽可能少地与其他实体发生相互作用；
   - 例如，在界面设计时，一个按钮会触发多个控件响应，不利于扩展，可以新增一个中间类，负责与各个控件交互，从而将各个控件之间解耦

## 模式分类

### 创建型模式

- 创建型模式，将对象的创建和使用分离，在使用对象时无须关心对象的创建细节，从而降低系统耦合度
- 每个创建型模式都要回答三个问题：
  ```
  - 创建什么    What
  - 由谁来创建  Who
  - 何时创建    When
  ```

#### 单例模式：确保对象唯一性

- 

#### 简单工厂模式

#### 工厂方法模式

#### 抽象工厂模式

### 结构型模式 - 用于组合

### 行为型模式 - 用于交互















---------------
Okay, that's a fantastic realization! Understanding that maintainability comes from good design is a crucial step. Design patterns are excellent tools to help you structure your code in proven, understandable, and flexible ways.

Let's break down how to approach learning and using design patterns in C++.

**Why Design Patterns Improve Maintainability**

Before diving in, let's quickly recap *why* patterns help:

1.  **Shared Vocabulary:** Developers familiar with patterns can quickly understand the *intent* behind a piece of code ("Oh, this is using the Strategy pattern").
2.  **Proven Solutions:** They represent solutions that have worked well for common problems over time. You don't have to reinvent the wheel.
3.  **Flexibility & Extensibility:** Many patterns make it easier to add new features or change existing ones without breaking everything (following principles like Open/Closed).
4.  **Reduced Coupling:** Patterns often help decouple components, meaning changes in one part of the system are less likely to affect others.
5.  **Improved Readability:** While initially adding some indirection, a well-applied pattern makes the overall structure and flow clearer once understood.

**The "Gang of Four" (GoF)**

The most famous resource is the book "Design Patterns: Elements of Reusable Object-Oriented Software" by Erich Gamma, Richard Helm, Ralph Johnson, and John Vlissides (often called the Gang of Four or GoF). They categorized patterns into three main types:

1.  **Creational Patterns:** Deal with object creation mechanisms, trying to create objects in a manner suitable to the situation.
2.  **Structural Patterns:** Deal with object composition, assembling objects and classes into larger structures while keeping these structures flexible and efficient.
3.  **Behavioral Patterns:** Deal with algorithms and the assignment of responsibilities between objects, focusing on communication between them.

**A Mini-Tutorial: Starting with Key Patterns**

Let's look at a few fundamental patterns with simple C++ examples. We'll focus on the *problem* they solve, the *solution* structure, and a basic implementation.

---

### 1. Creational Pattern: Factory Method

*   **Problem:** You need to create objects, but you want subclasses to be able to specify *which* concrete class to instantiate. You want to decouple the client code that *uses* the objects from the code that *creates* them. Imagine a document editor application that needs to create different document types (text, spreadsheet, etc.).
*   **Solution:** Define an interface (or abstract base class) for creating an object (the "factory method"), but let subclasses implement this method to return instances of different concrete classes.

```c++
#include <iostream>
#include <string>
#include <memory> // For std::unique_ptr

// ---- Product Interface ----
class Document {
public:
    virtual ~Document() = default; // Virtual destructor is important!
    virtual void open() = 0;
    virtual void save() = 0;
    // ... other document operations
};

// ---- Concrete Products ----
class TextDocument : public Document {
public:
    void open() override { std::cout << "Opening Text Document\n"; }
    void save() override { std::cout << "Saving Text Document\n"; }
};

class SpreadsheetDocument : public Document {
public:
    void open() override { std::cout << "Opening Spreadsheet Document\n"; }
    void save() override { std::cout << "Saving Spreadsheet Document\n"; }
};

// ---- Creator (Factory) ----
class Application {
public:
    virtual ~Application() = default;

    // The Factory Method! Subclasses MUST implement this.
    virtual std::unique_ptr<Document> createDocument() = 0;

    void newDocument() {
        // Use the factory method to create the product
        std::unique_ptr<Document> doc = createDocument();
        doc->open();
        // ... work with the document
    }
    // ... other application logic
};

// ---- Concrete Creators ----
class TextApplication : public Application {
public:
    std::unique_ptr<Document> createDocument() override {
        return std::make_unique<TextDocument>();
    }
};

class SpreadsheetApplication : public Application {
public:
    std::unique_ptr<Document> createDocument() override {
        return std::make_unique<SpreadsheetDocument>();
    }
};

// ---- Client Code ----
int main() {
    std::unique_ptr<Application> app;

    // Decide which application (and thus document type) to run
    bool useTextEditor = true; // Maybe based on user input or config
    if (useTextEditor) {
        app = std::make_unique<TextApplication>();
    } else {
        app = std::make_unique<SpreadsheetApplication>();
    }

    app->newDocument(); // Creates and opens the appropriate document type

    return 0;
}
```

*   **Maintainability Win:** If you want to add a `PresentationDocument`, you create the `PresentationDocument` class and a `PresentationApplication` class. The existing `Application` base class and the client code using `app->newDocument()` don't need to change.

---

### 2. Structural Pattern: Adapter

*   **Problem:** You have a class whose interface doesn't match the one you need, maybe because it's a third-party library or legacy code. You want to make it compatible without modifying the original class. Imagine you have a `LegacyRectangle` class with a `display()` method, but your system expects objects with a `draw()` method conforming to a `Shape` interface.
*   **Solution:** Create a new "Adapter" class that implements the target interface (`Shape`) and internally holds an instance of the class you want to adapt (`LegacyRectangle`). The Adapter's methods translate calls to the target interface into calls on the adaptee's interface.

```c++
#include <iostream>
#include <memory>

// ---- Adaptee (The class we want to use, but has the wrong interface) ----
class LegacyRectangle {
    int x1, y1, x2, y2;
public:
    LegacyRectangle(int x, int y, int w, int h) : x1(x), y1(y), x2(x + w), y2(y + h) {}

    void display() { // Old method name
        std::cout << "LegacyRectangle::display() from (" << x1 << "," << y1 << ") to (" << x2 << "," << y2 << ")\n";
    }
};

// ---- Target Interface (What the client code expects) ----
class Shape {
public:
    virtual ~Shape() = default;
    virtual void draw() = 0;
};

// ---- Adapter ----
// Inherits the Target interface, contains the Adaptee
class RectangleAdapter : public Shape {
private:
    // Can use composition (preferred) or private inheritance
    LegacyRectangle adaptee_;

public:
    RectangleAdapter(int x, int y, int w, int h) : adaptee_(x, y, w, h) {}

    void draw() override {
        std::cout << "RectangleAdapter::draw() calling adaptee -> ";
        adaptee_.display(); // Translate the call
    }
};

// ---- Client Code ----
void renderShape(Shape& shape) {
    shape.draw(); // Client code only knows about the Shape interface
}

int main() {
    // Create an adapter instance, wrapping the legacy object implicitly
    RectangleAdapter adaptedRect(10, 20, 100, 50);

    renderShape(adaptedRect); // Pass the adapter to the client code

    return 0;
}
```

*   **Maintainability Win:** You can integrate the `LegacyRectangle` without changing its code (which might be impossible if it's a library). The client code remains clean, only dealing with the `Shape` interface.

---

### 3. Behavioral Pattern: Strategy

*   **Problem:** You have an algorithm or behavior that can vary. You want to select the specific algorithm at runtime and be able to easily add new variations without changing the context class that uses the algorithm. Imagine a data compression utility that could use Zip, Rar, or 7z compression.
*   **Solution:** Define a family of algorithms, encapsulate each one in a separate class (the "Strategy" classes), and make them interchangeable. The context class holds a reference to a Strategy object and delegates the algorithmic task to it.

```c++
#include <iostream>
#include <vector>
#include <string>
#include <memory>
#include <algorithm> // For std::sort

// ---- Strategy Interface ----
class SortStrategy {
public:
    virtual ~SortStrategy() = default;
    virtual void sort(std::vector<int>& data) = 0;
};

// ---- Concrete Strategies ----
class BubbleSortStrategy : public SortStrategy {
public:
    void sort(std::vector<int>& data) override {
        std::cout << "Sorting using Bubble Sort\n";
        // Basic bubble sort implementation (inefficient, just for demo)
        size_t n = data.size();
        for (size_t i = 0; i < n - 1; ++i) {
            for (size_t j = 0; j < n - i - 1; ++j) {
                if (data[j] > data[j + 1]) {
                    std::swap(data[j], data[j + 1]);
                }
            }
        }
    }
};

class QuickSortStrategy : public SortStrategy {
public:
    void sort(std::vector<int>& data) override {
        std::cout << "Sorting using Quick Sort (using std::sort for simplicity)\n";
        std::sort(data.begin(), data.end()); // Use standard library quicksort
    }
};

// ---- Context ----
class DataContext {
private:
    std::vector<int> data_;
    std::unique_ptr<SortStrategy> strategy_; // Holds the current strategy

public:
    DataContext(std::unique_ptr<SortStrategy> strategy = nullptr) : strategy_(std::move(strategy)) {
        // Initialize with some data
        data_ = {5, 1, 4, 2, 8};
    }

    // Allow changing the strategy at runtime
    void setStrategy(std::unique_ptr<SortStrategy> strategy) {
        strategy_ = std::move(strategy);
    }

    void sortData() {
        if (strategy_) {
            strategy_->sort(data_); // Delegate sorting to the strategy object
            printData();
        } else {
            std::cout << "No sorting strategy set!\n";
        }
    }

    void printData() const {
        std::cout << "Data: ";
        for (int val : data_) {
            std::cout << val << " ";
        }
        std::cout << std::endl;
    }
};

// ---- Client Code ----
int main() {
    DataContext context;
    context.printData();

    // Use Bubble Sort
    context.setStrategy(std::make_unique<BubbleSortStrategy>());
    context.sortData();

    // Change strategy to Quick Sort
    context.setStrategy(std::make_unique<QuickSortStrategy>());
    context.sortData();

    // What if we want to add MergeSort?
    // 1. Create MergeSortStrategy class implementing SortStrategy.
    // 2. context.setStrategy(std::make_unique<MergeSortStrategy>());
    // The DataContext class doesn't need to change!

    return 0;
}
```

*   **Maintainability Win:** Adding a new sorting algorithm (`MergeSortStrategy`) only requires creating the new strategy class. The `DataContext` doesn't need modification (Open/Closed Principle). You can switch algorithms easily at runtime.

---

**How to Learn and Apply Patterns Effectively**

1.  **Start Simple:** Don't try to learn all 23 GoF patterns at once. Focus on the ones presented here (Factory Method, Adapter, Strategy) and maybe add Observer, Decorator, Singleton (but understand its drawbacks!), and Template Method next.
2.  **Understand the *Intent*:** More important than memorizing the UML diagram is understanding *what problem* the pattern solves and *why* its structure provides a good solution.
3.  **Look for Problems, then Patterns:** Don't start coding thinking "I must use a pattern here!". Instead, write your code, and when you encounter issues (hard to change, lots of `if/else` for types, tight coupling), ask yourself: "Is there a pattern that addresses this kind of problem?".
4.  **Refactor Towards Patterns:** A great way to learn is to take some of your existing, hard-to-maintain code and try to refactor it using an appropriate pattern. This makes the benefits concrete.
5.  **Study SOLID Principles:** Design patterns often embody principles like:
    *   **S**ingle Responsibility Principle (SRP)
    *   **O**pen/Closed Principle (OCP)
    *   **L**iskov Substitution Principle (LSP)
    *   **I**nterface Segregation Principle (ISP)
    *   **D**ependency Inversion Principle (DIP)
    Understanding SOLID will help you understand *why* patterns work and how to apply them correctly.
6.  **Use Modern C++:** Features like smart pointers (`std::unique_ptr`, `std::shared_ptr`), `virtual`/`override`, `final`, templates, and lambdas can make implementing patterns cleaner and safer in C++.
7.  **Don't Overuse Them:** Patterns add complexity (more classes, more indirection). Only use a pattern if the flexibility and maintainability benefits clearly outweigh this added complexity for your specific situation. Sometimes, a simpler solution is better.
8.  **Resources:**
    *   The GoF book (can be dense, maybe read after some online tutorials).
    *   Websites like Refactoring Guru, Sourcemaking, GeeksforGeeks have good explanations and examples.
    *   "Head First Design Patterns" (uses Java, but concepts are universal and explained very intuitively).

Start by trying to identify situations in your own code where one of these patterns might have helped. Could you use a Factory Method to handle object creation? Is there an awkward interface you could smooth over with an Adapter? Do you have `if/else` or `switch` statements choosing different behaviors that could become a Strategy?

Good luck! It's a journey, but learning design patterns will significantly level up your C++ design and coding skills.







--------------------
# 使用Unsloth微调


Okay, let's walk through fine-tuning the `Qwen/Qwen2-0.5B-Instruct` model (a good starting point, easily scalable to the 1.5B version) using Unsloth on an open-source dataset like `databricks/databricks-dolly-15k`. We'll use Google Colab for its free GPU access.

**Why Unsloth?**

*   **Speed:** Significantly faster training (up to 2x+) compared to standard Hugging Face implementations.
*   **Memory Efficiency:** Uses much less VRAM, allowing you to fine-tune larger models on smaller GPUs (like the free T4 on Colab) thanks to optimized kernels and efficient quantization (like 4-bit QLoRA).
*   **Ease of Use:** Integrates smoothly with the Hugging Face ecosystem (Transformers, PEFT, TRL, Datasets).

---

**Tutorial: Fine-tuning Qwen2-0.5B with Unsloth on Dolly-15k**

**1. Prerequisites**

*   A Google Account (for Google Colab) or a Kaggle Account.
*   Basic understanding of Python and Hugging Face libraries (Transformers, Datasets).
*   Familiarity with the concept of fine-tuning and LoRA (Low-Rank Adaptation).

**2. Setup Environment (Google Colab)**

*   Go to [Google Colab](https://colab.research.google.com/).
*   Create a new notebook.
*   **IMPORTANT:** Set the runtime type to use a GPU. Go to `Runtime` -> `Change runtime type` -> Select `T4 GPU` (or any available GPU).

**3. Install Libraries**

In a Colab cell, run the following command. Unsloth recommends installing its specific PyTorch version first, depending on your CUDA version (Colab T4 usually uses CUDA 12.1).

```bash
# Install Unsloth supporting CUDA 12.1, which is common on Colab
# Check your CUDA version with !nvcc --version if unsure
!pip install "unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git"

# Install other necessary libraries
!pip install "transformers>=4.41.0" "datasets>=2.16.0" "accelerate>=0.30.0" "trl>=0.8.3" "bitsandbytes>=0.43.0" "peft>=0.10.0"
```

*   **Note:** You might need to restart the runtime after these installations (`Runtime` -> `Restart runtime`).

**4. Load Model and Tokenizer with Unsloth**

Unsloth's `FastLanguageModel` handles loading the model with optimizations (like 4-bit quantization) and prepares it for LoRA automatically.

```python
import torch
from unsloth import FastLanguageModel
from transformers import AutoTokenizer

# --- Configuration ---
model_name = "Qwen/Qwen2-0.5B-Instruct" # You can change this to Qwen/Qwen2-1.5B-Instruct or a base model
max_seq_length = 2048 # Choose based on GPU memory and dataset; Qwen2 supports long contexts
dtype = None # None for auto detection (will use bfloat16 if available, float16 otherwise)
load_in_4bit = True # Use 4-bit quantization for memory efficiency

# --- Load Model ---
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = model_name,
    max_seq_length = max_seq_length,
    dtype = dtype,
    load_in_4bit = load_in_4bit,
    # token = "hf_...", # Add your Hugging Face token if loading private/gated models
)

print("Model and Tokenizer Loaded.")
print(f"Model Device: {model.device}")
print(f"Model Dtype: {model.dtype}") # Should show torch.bfloat16 or torch.float16 if quantized
```

**5. Prepare for LoRA Fine-tuning**

Unsloth automatically adds LoRA adapters to the model. We use PEFT (Parameter-Efficient Fine-Tuning) configurations here. `FastLanguageModel` applies sensible defaults, but you can customize them.

```python
from peft import LoraConfig

# Configure LoRA using PEFT
# Unsloth automatically finds target modules, but you can specify if needed
model = FastLanguageModel.get_peft_model(
    model,
    r = 16, # LoRA rank (dimension). Recommended: 8, 16, 32. Higher = more params but potentially better performance.
    lora_alpha = 32, # Scaling factor. Recommended: r * 2
    target_modules = ["q_proj", "k_proj", "v_proj", "o_proj",
                      "gate_proj", "up_proj", "down_proj"], # Modules to apply LoRA to. Unsloth finds these automatically usually.
    lora_dropout = 0.05, # Dropout probability for LoRA layers
    bias = "none",    # Set bias usage. 'none' is common.
    use_gradient_checkpointing = True, # Saves memory during training
    random_state = 3407,
    use_rslora = False,  # Rank-Stabilized LoRA (experimental)
    loftq_config = None, # LoftQ configuration (experimental)
)

print("PEFT Model Prepared for LoRA.")
model.print_trainable_parameters() # See how many parameters are actually trainable (should be small %)
```

**6. Load and Prepare the Dataset**

We'll use the `databricks/databricks-dolly-15k` dataset, which has columns like `instruction`, `context`, and `response`. We need to format it into a single text string that the model can learn from. Qwen2-Instruct uses a specific chat template.

```python
from datasets import load_dataset

# --- Dataset Configuration ---
dataset_name = "databricks/databricks-dolly-15k"

# --- Load Dataset ---
dataset = load_dataset(dataset_name, split="train")

# --- Formatting Function ---
# We need to structure the data into the format Qwen2-Instruct expects.
# Ref: https://huggingface.co/Qwen/Qwen2-0.5B-Instruct#chat-template
# Example format:
# <|im_start|>system
# You are a helpful assistant.<|im_end|>
# <|im_start|>user
# {instruction}{optional_context}<|im_end|>
# <|im_start|>assistant
# {response}<|im_end|>

def formatting_prompts_func(examples):
    instructions = examples["instruction"]
    inputs       = examples["context"]
    outputs      = examples["response"]
    texts = []
    for instruction, input_text, output in zip(instructions, inputs, outputs):
        # Create the prompt pieces
        messages = [
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": f"{instruction}\n{input_text}" if input_text else instruction},
            {"role": "assistant", "content": output}
        ]
        # Apply the chat template without tokenizing yet
        formatted_text = tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=False # We already have the assistant's response
        )
        texts.append(formatted_text)
    return { "text" : texts, } # SFTTrainer expects a 'text' column

# Apply formatting
dataset = dataset.map(formatting_prompts_func, batched = True,)

print("Dataset Loaded and Formatted.")
print("Example formatted text:")
print(dataset[0]['text'])
```

**7. Configure Training Arguments**

Set up the training parameters using Hugging Face's `TrainingArguments`. Unsloth works seamlessly with it and adds optimized options like `optim="adamw_8bit"`.

```python
from transformers import TrainingArguments

# --- Training Configuration ---
output_dir = "qwen2-0.5b-dolly-unsloth" # Directory to save the LoRA adapters
batch_size = 8 # Adjust based on GPU memory (try 2, 4, 8, 16)
gradient_accumulation_steps = 4 # Effective batch size = batch_size * gradient_accumulation_steps
learning_rate = 2e-4 # Common learning rate for LoRA
num_train_epochs = 1 # Number of passes through the dataset (1-3 is common for fine-tuning)
# max_steps = -1 # Alternatively, set max_steps instead of epochs (-1 means use epochs)
logging_steps = 10 # How often to log training progress
save_steps = 50 # How often to save checkpoints (LoRA adapters)
warmup_ratio = 0.1 # Proportion of training steps for learning rate warmup
lr_scheduler_type = "linear" # Learning rate schedule
optim = "adamw_8bit" # Use Unsloth's 8-bit AdamW optimizer for memory savings

training_args = TrainingArguments(
    per_device_train_batch_size = batch_size,
    gradient_accumulation_steps = gradient_accumulation_steps,
    warmup_ratio = warmup_ratio,
    num_train_epochs = num_train_epochs,
    # max_steps = max_steps,
    learning_rate = learning_rate,
    fp16 = not torch.cuda.is_bf16_supported(), # Use fp16 if bf16 not supported
    bf16 = torch.cuda.is_bf16_supported(), # Use bf16 if supported (more stable)
    logging_steps = logging_steps,
    optim = optim,
    weight_decay = 0.01,
    lr_scheduler_type = lr_scheduler_type,
    seed = 3407,
    output_dir = output_dir,
    save_strategy = "steps", # Save based on steps
    save_steps = save_steps,
    save_total_limit = 3, # Keep only the latest 3 checkpoints
    report_to = "tensorboard", # Optional: use "wandb" if you have it configured
    # dataloader_num_workers=2 # Speeds up data loading (adjust based on system)
)

print("Training Arguments Configured.")
```

**8. Initialize and Start Trainer**

Use TRL's `SFTTrainer` (Supervised Fine-tuning Trainer), which is optimized by Unsloth automatically.

```python
from trl import SFTTrainer

trainer = SFTTrainer(
    model = model,
    tokenizer = tokenizer,
    train_dataset = dataset,
    dataset_text_field = "text", # The column name containing our formatted text
    max_seq_length = max_seq_length,
    dataset_num_proc = 2, # Number of processes for dataset mapping
    packing = False, # Optional: pack multiple sequences into one - can speed up training but complex
    args = training_args,
)

print("Trainer Initialized. Starting Training...")

# --- Start Training ---
trainer_stats = trainer.train()

print("Training Finished!")
print(trainer_stats)
```

**9. Inference with the Fine-tuned Model**

Now, let's test the fine-tuned model. The `model` object in your session *is* the LoRA-adapted model.

```python
# --- Inference Setup ---
# Use the model object directly if training just finished in the same session.
# If loading later, you'd load the base model + adapter:
# from peft import PeftModel
# base_model, tokenizer = FastLanguageModel.from_pretrained(model_name, ...) # Load base again
# model = PeftModel.from_pretrained(base_model, output_dir) # Load adapter

# Prepare prompt using the chat template
messages = [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "What are the main challenges of space exploration?"}
]

# Use Unsloth's fast inference endpoint
inputs = tokenizer.apply_chat_template(
    messages,
    tokenize=True,
    add_generation_prompt=True, # Add the prompt for the assistant to start responding
    return_tensors="pt"
).to(model.device) # Ensure inputs are on the same device as the model

# --- Generate Response ---
# Use FastLanguageModel's built-in generation for optimized inference
outputs = model.generate(
    input_ids=inputs,
    max_new_tokens=256,
    use_cache=True, # Important for speed
    pad_token_id=tokenizer.eos_token_id, # Set pad token for open-ended generation
    eos_token_id=tokenizer.eos_token_id,
    do_sample=True, # Use sampling for more creative responses
    temperature=0.7,
    top_p=0.9,
)

# Decode the generated tokens
response = tokenizer.batch_decode(outputs[:, inputs.shape[1]:], skip_special_tokens=True)[0] # Decode only the new tokens

print("--- Prompt ---")
print(tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True))
print("\n--- Response ---")
print(response)

# --- Example 2: Using context ---
messages_context = [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "Based on the following text about photosynthesis:\nPhotosynthesis is a process used by plants and other organisms to convert light energy into chemical energy that, through cellular respiration, can later be released to fuel the organisms' activities.\n\nExplain photosynthesis in simple terms."}
]

inputs_context = tokenizer.apply_chat_template(messages_context, tokenize=True, add_generation_prompt=True, return_tensors="pt").to(model.device)
outputs_context = model.generate(input_ids=inputs_context, max_new_tokens=100, use_cache=True, pad_token_id=tokenizer.eos_token_id, eos_token_id=tokenizer.eos_token_id, do_sample=True, temperature=0.6, top_p=0.9)
response_context = tokenizer.batch_decode(outputs_context[:, inputs_context.shape[1]:], skip_special_tokens=True)[0]

print("\n--- Prompt with Context ---")
print(tokenizer.apply_chat_template(messages_context, tokenize=False, add_generation_prompt=True))
print("\n--- Response ---")
print(response_context)
```

**10. Saving the Model (LoRA Adapters)**

The trainer automatically saves checkpoints, but you can also save the final adapters manually.

```python
# Save final LoRA adapters
final_adapter_dir = f"{output_dir}/final_adapter"
model.save_pretrained(final_adapter_dir)
tokenizer.save_pretrained(final_adapter_dir)

print(f"Final LoRA adapters saved to: {final_adapter_dir}")

# Optional: Push to Hugging Face Hub
# model.push_to_hub(f"your_username/{output_dir}", token="hf_...")
# tokenizer.push_to_hub(f"your_username/{output_dir}", token="hf_...")
```

**11. Optional: Merging Adapters (for easier deployment)**

You can merge the LoRA adapters into the base model weights. This creates a larger model file but simplifies inference as you don't need PEFT anymore. *Warning:* This increases VRAM usage significantly compared to using adapters.

```python
# Make sure you have enough RAM/VRAM to merge!
# merged_model = model.merge_and_unload()

# Save the merged model
# merged_model_dir = f"{output_dir}/final_merged_model"
# merged_model.save_pretrained(merged_model_dir)
# tokenizer.save_pretrained(merged_model_dir)
# print(f"Merged model saved to: {merged_model_dir}")
```

---

**Key Considerations & Next Steps:**

*   **Model Choice:** Easily switch `model_name` to `Qwen/Qwen2-1.5B-Instruct`, `Qwen/Qwen2-0.5B` (base), `Qwen/Qwen2-1.5B` (base), or other Unsloth-supported models. Remember that larger models require more VRAM.
*   **Base vs. Instruct:** Base models are trained on raw text and are good for further pre-training or fine-tuning for specific non-chat tasks. Instruct models are already fine-tuned for following instructions and chat, making them better starting points for chatbots or assistants. Remember to adjust the `formatting_prompts_func` if you use a base model (it won't understand the chat template).
*   **Dataset:** Choose a dataset relevant to your task. Ensure your `formatting_prompts_func` correctly structures the data for the model. If your dataset already has a "text" column with pre-formatted prompts, you can skip the `.map()` step and directly use `dataset_text_field="text"` in `SFTTrainer`.
*   **Hyperparameters:** Experiment with `learning_rate`, `batch_size`, `num_train_epochs`, `r`, `lora_alpha` to optimize performance.
*   **Evaluation:** Add an evaluation step using a separate validation dataset to objectively measure your fine-tuned model's performance.
*   **Memory:** Monitor GPU memory usage (`!nvidia-smi`). If you run out of memory, decrease `batch_size`, `max_seq_length`, or `r`. Use `gradient_accumulation_steps` to maintain a larger effective batch size.

This tutorial provides a solid foundation for fine-tuning LLMs efficiently using Unsloth. Good luck!