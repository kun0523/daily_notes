# NN

# CNN
## 特征图尺寸计算
- 高度尺寸变化：
$H_2 = \frac{H_1-F_H+2P}{S} + 1$
- 宽度尺寸变化：
$W_2 = \frac{W_1-F_W+2P}{S} + 1$

- $H_1 \ W_1 \ H_2 \ W_2$ 分别指 卷积前后的尺寸
- $F_H \ F_W$ 分别指 卷积核的尺寸
- $P \ S$ 分别指 Padding（指一侧的大小） 和 Stride 大小

## 卷积参数量计算
- 卷积核参数 = $F_h*F_w*N_{chl}*N_{filter}$
- 偏置 = 卷积核个数 = $N_{filter}$

## 池化
- 通常不做 `Padding`
- 通常 `Stride = FilterSize`
- 通常 池化卷积核 数量更多，**使特征图的个数翻倍**；
- 计算特征图的方式同上

## 怎么数有多少层？
- 像 `ReLU`  `Pooling` 层没有可训练的参数，不算一层；
- 仅 `Conv`  `FC`  层，有可训练参数的层计数；

## 卷积优势与劣势：
- 优势：参数少，训练速度快；（1个7x7的卷积核 比 3个3x3的卷积核 参数量更多）
- 劣势：缺乏不同位置特征的关联性；




# RNN

# Transformer

## 位置编码

- 因为每个token都有初始的词向量，同一个词出现在句子中的不同位置对应的词向量一致很不合理，所以需要加入位置编码，**使得同一个词出现在不同位置时，其向量不一致**；
- 位置编码直接与词向量做加法；

## Multi-headed机制

- 类似卷积中的多个卷积核，构建更丰富的特征；
- 多头得到的特征向量做拼接；
  
## Attention计算流程

### SelfAttention

![attention flow](..\image_resources\image-attention.png)

- 经过Attention计算后，特征的维度不变（多头注意力拼接，使最后的特征维度不变）；

### CrossAttention

- 应用在解码器中；
- 编码器提供 `key` 和 `value` 向量，解码器提供 `query` 向量；
- `输出token`的 `query` 依次跟 `输入token` 的 `key` 计算权重，然后对`输入token`的`value`做加权平均；
![CrossAttention](..\image_resources\image_crossattention.png)
