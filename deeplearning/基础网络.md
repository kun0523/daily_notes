# NN

# CNN


# RNN

# Transformer

## 位置编码

- 因为每个token都有初始的词向量，同一个词出现在句子中的不同位置对应的词向量一致很不合理，所以需要加入位置编码，**使得同一个词出现在不同位置时，其向量不一致**；
- 位置编码直接与词向量做加法；

## Multi-headed机制

- 类似卷积中的多个卷积核，构建更丰富的特征；
- 多头得到的特征向量做拼接；
  
## Attention计算流程

### SelfAttention

![attention flow](..\image_resources\image-attention.png)

- 经过Attention计算后，特征的维度不变（多头注意力拼接，使最后的特征维度不变）；

### CrossAttention

- 应用在解码器中；
- 编码器提供 `key` 和 `value` 向量，解码器提供 `query` 向量；
- `输出token`的 `query` 依次跟 `输入token` 的 `key` 计算权重，然后对`输入token`的`value`做加权平均；
![CrossAttention](..\image_resources\image_crossattention.png)
