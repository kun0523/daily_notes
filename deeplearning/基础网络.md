# NN

# CNN
## 特征图尺寸计算
- 高度尺寸变化：
$H_2 = \frac{H_1-F_H+2P}{S} + 1$
- 宽度尺寸变化：
$W_2 = \frac{W_1-F_W+2P}{S} + 1$

- $H_1 \ W_1 \ H_2 \ W_2$ 分别指 卷积前后的尺寸
- $F_H \ F_W$ 分别指 卷积核的尺寸
- $P \ S$ 分别指 Padding（指一侧的大小） 和 Stride 大小

## 卷积参数量计算
- 卷积核参数 = $F_h*F_w*N_{chl}*N_{filter}$
- 偏置 = 卷积核个数 = $N_{filter}$

## 池化
- 通常不做 `Padding`
- 通常 `Stride = FilterSize`
- 通常 池化卷积核 数量更多，**使特征图的个数翻倍**；
- 计算特征图的方式同上

## 怎么数有多少层？
- 像 `ReLU`  `Pooling` 层没有可训练的参数，不算一层；
- 仅 `Conv`  `FC`  层，有可训练参数的层计数；

## 卷积优势与劣势：
- 优势：参数少，训练速度快；（1个7x7的卷积核 比 3个3x3的卷积核 参数量更多）
- 劣势：缺乏不同位置特征的关联性；




# RNN

# Transformer

## 位置编码

- 因为每个token都有初始的词向量，同一个词出现在句子中的不同位置对应的词向量一致很不合理，所以需要加入位置编码，**使得同一个词出现在不同位置时，其向量不一致**；
- 位置编码直接与词向量做加法；

## Multi-headed机制

- 类似卷积中的多个卷积核，构建更丰富的特征；
- 多头得到的特征向量做拼接；
  
## Attention计算流程

### SelfAttention

![attention flow](..\image_resources\image-attention.png)

- 经过Attention计算后，特征的维度不变（多头注意力拼接，使最后的特征维度不变）；

### CrossAttention

- 应用在解码器中；
- 编码器提供 `key` 和 `value` 向量，解码器提供 `query` 向量；
- `输出token`的 `query` 依次跟 `输入token` 的 `key` 计算权重，然后对`输入token`的`value`做加权平均；
![CrossAttention](..\image_resources\image_crossattention.png)



# 评测指标

## 交叉熵 与 KL散度

### 熵

- 信息量： 
  - 一个事件发生的概率越低，它所包含的信息量越大；
  - $I(x) = -\log{P(x)}$
  - 对数以2为底时，单位为bit；对数以e为底时，单位为奈特 nat
- 熵：
  - 衡量一个概率分布的不确定性或平均信息量
  - 它是该分布下所有可能事件信息量的**期望值**
  - $H(P) = E(I(x))=-\sum{P(x)logP(x)}$

### 交叉熵
- 当使用估计分布Q 的最优编码方案 去编码来自真实分布P的样本时，平均所需的编码长度
- $H(P, Q) = -\sum{P(x)\log{Q(x)}}$
- 可以用来衡量两个分布之间的差异大小

### KL散度
- 相对熵
- 衡量的是，使用分布Q 来近似 分布P 时，所**损失的信息量**（两个分布之间的差异）
- $D(P||Q)=\sum{P(x)\log{(P(x)/Q(x))}}=\sum{P(x)[logP(x)-logQ(x)]}=\sum{P(x)logP(x)}-\sum{P(x)logQ(x)}=H(P,Q)-H(P)$

### 交叉熵与KL散度的区别
- 分类问题中，常使用交叉熵作为目标函数，主要是因为**最小化交叉熵等价于最小化KL散度**（预测标签概率分布与真实标签概率分布的差异）


## BELU
