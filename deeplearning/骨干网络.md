# 核心部分

- 网络结构
- 损失函数
- 参数优化算法
- 如何处理过拟合等中间问题

# ResNet

# 可变形卷积

- 卷积不再是计算相邻区域，而是多算 x y 方向的偏移量之后的卷积
- 使得卷积更多集中计算前景区域
- 适用于有很多遮挡物的场景；

# 可变形注意力机制

- 因为注意力机制，在计算`self-attention`时计算量特别大，因为要和每个头两两计算注意力；
- 提出可变形注意力，即多预测两个值，只关注4个头之间的注意力，从而降低计算量；

# SqueezeNet

# DarkNet

# CSPNet

# ViT

- `CNN` 卷积是通过卷积核提取区域特征，通过多层堆叠获取更大的感受野，从而获得全局语义信息；
- `Transformer` 第一步使用CNN提取区域特 m征，然后用`Self-Attention`机制提取不同区域之间的关系，从而获得全局语义信息；
- `注意力机制`是通过`Q K V` 三个矩阵实现的，其中 `Q K V` 分别是通过`MLP`全连接网络，从`Token`经过`Embedding` 得到的
- `多头注意力机制` 是将 `Q K V` 的特征维度，拆分成多份，每个头训练一部分，最好`concate`起来；

# DINO

- https://medium.com/@anuj.dutt9/emerging-properties-in-self-supervised-vision-transformers-dino-paper-summary-4c7a6ed68161

- Self-DIstillation with NO Labels
- 基于对比学习  Contrastive Learning

- 训练方法
  - 多重裁剪策略：采用多种数据集增强技术，同一张图产生多种变形后的视角；
  - Local-to-Global Learning：所有裁剪的图传入 Student Model ，Global view 传入 Teacher Model；<font color=red>让Student Model学习 Local 图 与 Global 图的关系？？？</font>
  - 损失：最小化 Teacher Model 和 Student Model 之间的， 针对同一张图片，不同视角下的表征偏差，所以不需要标签；
  - ![image-20240617103652196](..\image_resources\image-20240617103652196.png)
  - <img src="..\image_resources\image-20240617103652196.png" style="zoom:80%;">
  - Teacher Model 和 Student Model 有相同的结构，但是不同的参数；
  - 传入两个模型的是，同一张图片的不同随机增强的结果；
  - Teacher Model 的参数 是根据 Student Model 参数的指数滑动平均（ema）结果进行更新的，Student Model 是根据反向传播更新的；
  - 

# DCN

- Dual-channel Convolutional Network  基于双通道的卷积神经网络