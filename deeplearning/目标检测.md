- 模型计算量：

  - 深度学习模型的计算量通常指模型在一次推理（inference）或者一个训练迭代（training iteration）中所需进行的总浮点运算次数（FLOPs，Floating Point Operations）。
  - 假设有一个深度学习模型在一次前向传播中需要进行 5 亿次浮点运算（500 MFlops）。如果运行环境的计算能力是每秒 2 GFlops，那么这个环境大约能够在 500/2000=0.25500/2000=0.25 秒内完成一次前向传播。反之，如果训练一个模型每批次需要执行 10 GFlops 的运算量，使用一个每秒 5 GFlops 的计算设备，大约需要 2 秒完成一批次的训练。
  - 1. **GFlops（GigaFlops）**:
       - 1 GFlop 表示每秒执行十亿次（109109）浮点运算。
       - GFlops 是 "Giga Floating Point Operations per Second" 的缩写。
  - 2. **TFlops（TeraFlops）**:
       - 1 TFlop 表示每秒执行一万亿次（10121012）浮点运算。
       - TFlops 是 "Tera Floating Point Operations per Second" 的缩写。

### 计算实例

- 如果一块 GPU 有 5 TFlops 的计算能力，那么它在一秒钟内可以执行 5×10125×1012 次浮点运算。
- 如果一个神经网络模型在训练过程中需要每秒进行 500 GFlops 的运算量，那么如此 GPU 或系统需要至少具备 0.5 TFlops 的计算能力来满足这个需求。

### 示例代码

- 如果你想在程序中估计或者计算设备的 Flops，可以使用一些基准测试或者分析工具。例如，在深度学习框架中如 PyTorch 或 TensorFlow 可以使用一些内建的工具来测量模型的浮点运算量。

    以 PyTorch 为例：

    ```python
    import torch
    import torch.nn as nn
    
    model = nn.Sequential(
        nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),
        nn.ReLU(),
        nn.MaxPool2d(kernel_size=2, stride=2),
        nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),
        nn.ReLU(),
        nn.MaxPool2d(kernel_size=2, stride=2),
    )
    
    # Example input of the model
    input = torch.randn(1, 3, 224, 224)
    
    # Use torchinfo summary to get the number of FLOPs
    from torchinfo import summary
    summary(model, input_size=(1, 3, 224, 224))
    ```

    总之，GFlops 和 TFlops 提供了计算设备性能和深度学习计算需求的一个度量标准，通过理解这些单位的具体含义和应用情景，可以更好地设计、选择和使用计算设备。

    

- 将第三方库安装在指定路径下，并在脚本中引入
  
  - `pip install beautifulsoup4 -t /path/to/third_party_lib`
  - `import sys   sys.path.append('/path/to/third_party_lib')` 
  - ​	
  
- 学习率
  - 探索学习率，最小为默认学习率的 1/10, 最大为默认学习率的 5 倍
  - 成倍增加显卡数量时，学习率也应该线性增加，例如，单卡学习率 0.1  换成4卡应改为 0.4；换成8卡应改为0.8
  
- lr/pg0

- lr/pg1

- lr/pg2

# 超参数配置

## 恢复训练 resume
- `yolo train resume model=path/to/last.pt` 
  - 从 last.pt 文件中加载 之前的训练参数、优化器状态、学习率规划
  - 日志文件还记录在之前的日志文件中


## 数据增强

- 无数据增强时，仅将图片resize到指定尺寸（保持比例，边缘padding的方式）
- 公用
  - mixup：将两张图片叠加到一起；
  - mosaic：多张图片拼接
  - scale：缩放， +-0~1，产品比例
  - degrees：旋转角度，+- 角度值；<img src="..\image_resources\train_batch0.jpg" alt="train_batch0" style="zoom: 20%;" />
  - translate：平移图片，+- 0~1，移动产品大小的比例；
  - shear：<img src="..\image_resources\train_batch2.jpg" alt="train_batch2" style="zoom:20%;" />



- 检测
  - 
- 分割
  - copy_paste
- 分类
  - auto_augment
  - erasing
  - crop_fraction



# 分类

## 案例： 包装状态检查

### 训练

- 生成config
  - `yolo classify train model=yolov8l-cls.pt data=E:\DataSets\vacuum_package\used`
- 使用已有config
  - `yolo cfg=D:\share_dir\repository_check\workdir_v8_0325\cls_large_0325.yaml`
- 模型导出
  - `yolo export model=path/to/best.pt format=onnx`
  - `yolo export format=onnx opset=12 model=path/to/best.pt `
  - `yolo export format=onnx model=path/to/best.pt --dynamic --half`
  - ？？？？pytorch 模型默认使用 FP32  ONNX 模型默认使用 FP16  所以导出onnx后模型文件会比较大，可以自己指定导出FP16模型  （指定half后还是没有变小）
- 学习率
  - lr/pg0  记录backbone权重的学习率
  - lr/pg1  记录YOLO layers 的学习率
  - lr/pg2  记录其他附加参数的学习率 例如 biases
- 迁移学习
  - freeze 冻结网络层 `freeze 10` 冻结backbone 仅训练head
  - 这里的数字是指block，并不是卷积层，从模型config文件中可以查到，ultralytics backbone 是0~9，这里写10，是指backbone都不参与训练
  - freeze 之后可以**节省显存**，**加速训练速度**

### 验证

- 在基线数据集上验证模型效果

  - `yolo classify val model=path/to/best.pt data=path/to/data.yaml`

  - ```yaml
    # Train/val/test sets as 1) dir: path/to/imgs, 2) file: path/to/imgs.txt, or 3) list: [path/to/imgs1, path/to/imgs2, ..]
    path: E:\DataSets\vacuum_package\used # dataset root dir
    train: ./train # train images (relative to 'path') 4 images
    val: ./val # val images (relative to 'path') 4 images
    test: ./test # test images (optional)
    
    # Classes
    names:
      0: GREAT
      1: NG_OUT
      2: NG_UNSEAL
      3: OK
      4: UNKNOW
    ```

  - 

### 推理
- 单张图片推理
- batch推理
- 

### 模型效果 GradCAM



- 注意图片通道转换问题！！！

  - ```python
    # tensor 3*224*224  --> 224*224*3 
    result_np = result.permute(1,2,0).mul(255).numpy().astype(np.uint8)
    # permute(2,1,0) 会导致图像逆时针转90度
    result_np = result.permute(2,1,0).mul(255).numpy().astype(np.uint8)
    
    ```

  - 





# 检测

## 常用指标

### mAP50

- Mean Average Precision at 50% overlap
- 当det bbox 与 gt bbox IOU>50% 时，所有类别的平均Precision
- 先计算各个类别的AP，然后计算所有类别的平均AP
- AP代表Precision-Recall 曲线与x轴围成的面积
- 数值越趋近于1越好；

### mAP50-95

- Mean Average Precision from 50% to 95% overlap
- 相比 mAP50, mAP50-95 考虑IoU 从50%到95%(每隔5%计算一次)，可以更好的评估模型的稳健程度；

- 在评测指标方面，目标检测常用的评测指标有 mAP@0.5 和 mAP@[0.5:0.95]，mAP@0.5 是指目标检测器在交并比(IOU)阈值为 0.5 的条件下在数据集各个类别上的平均精度，mAP@[0.5:0.95] 是指目标检测器在 IOU=0.5 至 IOU=0.95 以 5% 为步长得到的不同阈值为条件而得到的平均精度，mAP@[0.5:0.95] 通常更加侧重反映的是目标检测器的位置检测精度，而在该任务的场景中，我们更加关注目标的检出准确性，需要保证能够有效的防止出现漏检或者误检的情况，因此我们**采用 mAP@0.5 作为检测器的精度评测指标**。

## 损失函数

- 只有一个类别时，样本不均是指，目标类别和背景类别比例差别大

### box_loss

- Smooth L1 loss
- Mean Squared Error
- 标注坐标 `(tlx, tly, brx, bry)` 
- 在YOLO框架下，box_loss 是通过 CIOU(Complete Intersection over Union) 计算的，不仅考虑IOU，还考虑两个框的长宽比是否相近

### cls_loss

- cross entropy loss
- binary cross entropy loss
- 预测类别与真实类别的差异

### dfl_loss

- Distribution Focal Loss  分布式 Focal Loss
- yolov8 中 使用 dfl 促进 bbox 框的准确性？？

### focal_loss

- 用于解决正负样本不均衡问题

- 有RetinaNet 在 2017年提出的；

- 算法思想：降低简单样本的训练权重，提高困难样本的训练权重

- $FL(p_t)=−(1−p_t)^γ⋅log(p_t)$

  where:

  - $p_t$ 预测为正确类别的概率

  - $γ$  聚焦参数，通常取值（0~5）控制多大程度上降低简单的样本的权重

    
  
- 

  



## 案例：PCB区域检测

### 数据标注
- `class_id center_x center_y width height`
- bbox 中心坐标 宽高 除以图片的宽高进行归一化

#### 数据标注错误
- 如果将数据错误标注成 `class_id tlx tly brx brh`  在使用ultralytics框架时，train batch可视化会有显著差异

### 训练

- 第一次训练：还没有生成config文件
  - `yolo train model=yolov8n.pt data=path/to/data_config.yaml` 
- 生成config.yaml后就可以自行修改配置文件
  - `yolo cfg=path/to/train_config.yaml`
- 导出onnx模型
  - `yolo export opset=12 format=onnx model=path/to/best.pt` 

### 验证

### 推理

- `yolo predict model=path_to_model.pt source=path_to_images_dir` 

#### 使用python YOLO 推理

##### 图像前处理

- LetterBox  Resize image and padding for detection
  - 算法流程：
  - 1. 最小缩放比：`r=min(new_shape[0]/old_shape[0], new_shape[1]/old_shape[1])` 
    2. 计算padding：

##### 结果后处理

- 模型返回值  `batch_size * (4 coordinates + num_cls) * 8400`
- **4 coordinates: center_x, center_y, width, height**  



#### 使用openvino推理

- **图片前处理**
- **模型返回结果的后处理**

- 模型返回结果： `1*7*8400` 
  
- 7: 前四个表示坐标  x, y, w, h; 后面几位代表各个类别的置信度
  
- cpp

- ```cpp
  #include <iostream>
  #include <opencv2/opencv.hpp>
  #include <openvino/openvino.hpp>
  #include <algorithm>
  
  using std::cout;
  using std::endl;
  using std::string;
  using std::vector;
  
  int main(){
      string onnx_pth = R"(D:\share_dir\pd_mix\workdir\det_pcb\train\weights\best.onnx)";
      ov::Core core;
      auto compiled_model = core.compile_model(onnx_pth, "CPU");
      ov::InferRequest infer_request = compiled_model.create_infer_request();
  
      string image_pth = R"(E:\DataSets\pd_mix\test_small_pic\20240407_00001_P51-L_A741427074_13_17_C1_FK_A2FK2S42HOICD123_A2FK2S4338MCD090.jpg)";
      cv::Mat img = cv::imread(image_pth);
      int org_img_h = img.rows, org_img_w = img.cols;
      cout <<"original image size: " << img.size << endl;
      cv::Mat blob_img = cv::dnn::blobFromImage(img, 1.0/255.0, cv::Size(640, 640), cv::Scalar(0.0,0.0,0.0), true, false, CV_32F);
      cout << "blob size: " << blob_img.size << endl;
  
      auto input_port = compiled_model.input();
      ov::Tensor input_tensor(input_port.get_element_type(), input_port.get_shape(), blob_img.ptr());
      infer_request.set_input_tensor(input_tensor);
      infer_request.infer();
  
      // 1*7*8400  cx, cy, w, h, c1, c2, c3...
      ov::Shape output_tensor_shape = compiled_model.output().get_shape();
      size_t batch=output_tensor_shape[0], preds=output_tensor_shape[1], bbox_num=output_tensor_shape[2];
      cout << output_tensor_shape << endl;
      auto output_tensor = infer_request.get_output_tensor();
      const float* output_buff = output_tensor.data<const float>();
  
      cv::Mat m = cv::Mat(cv::Size(bbox_num, preds), CV_32F, const_cast<float*>(output_buff));
      m = m.t();
      cout <<"Transpose: " << m.size << endl;
  
      float w_factor{float(org_img_w)/640}, h_factor{float(org_img_h)/640};
      float score_threshold = 0.5;
      float iou_threshold = 0.5;
      vector<cv::Rect> boxes;
      vector<float> scores;
      vector<int> indices;
      for(int row=0; row<m.size[0]; ++row){
          float* ptr = m.ptr<float>(row);
          float cx = ptr[0], cy = ptr[1], w=ptr[2], h=ptr[3];
          vector<float> cls_conf = vector<float>(ptr+4, ptr+7);
          cv::Point maxP;
          double maxV;
          cv::minMaxLoc(cls_conf, 0, &maxV, 0, &maxP);
          boxes.push_back(cv::Rect2d(cx-w/2, cy-h/2, w, h));
          scores.push_back(static_cast<float>(maxV));
      }
  
  
      cv::dnn::NMSBoxes(boxes, scores, score_threshold, iou_threshold, indices);
      cout << "pred bbox num:  " << indices.size() << endl;
  
      for(auto it=indices.begin(); it!=indices.end(); ++it){
          cv::Rect2d tmp = boxes[*it];
          tmp.x *= w_factor;
          tmp.y *= h_factor;
          tmp.width *= w_factor;
          tmp.height *= h_factor;
          cv::rectangle(img, tmp, cv::Scalar(0,0,255), 10);
      }
      if(cv::imwrite("test.jpg", img))
          cout << "Save Success." << endl;
  }
  ```

- 



### 模型效果 GradCAM



# 分割	

![../../_images/anatomy.png](https://matplotlib.org/stable/_images/anatomy.png)

## 数据标注形式

- 每张图片一个txt标注文件，标注文件与图片同名仅后缀不同；
- 在标注文件夹`.txt`中一个对象一行数据；
- 一行数据中包含：对象类别，多边形边界坐标，归一化[0,1]  使用空格分隔
- `<class-index> <x1> <y1> <x2> <y2> ... <xn> <yn>`
- 每个对象的多边形坐标长度不一定 一致；
- 一个对象的多边形坐标最少是三角形，即3个xy坐标对；

### coco格式转换

- 从 `COCO` 标注格式转换为 `YOLO` 标注格式

- ```python
  from ultralytics.data.converter import convert_coco
  
  convert_coco(labels_dir='path/to/coco/annotations/', use_segments=True)
  ```

- 

## 数据集描述文件 YAML 格式

```yaml
# Train/val/test sets as 1) dir: path/to/imgs, 2) file: path/to/imgs.txt, or 3) list: [path/to/imgs1, path/to/imgs2, ..]
path: ../datasets/coco8-seg  # dataset root dir
train: images/train  # train images (relative to 'path') 4 images
val: images/val  # val images (relative to 'path') 4 images
test:  # test images (optional)

# Classes (80 COCO classes)
names:
  0: person
  1: bicycle
  2: car
  # ...
  77: teddy bear
  78: hair drier
  79: toothbrush
```



## 常用指标

- Box mAP50
- Box mAP50-95
- Seg mAP50
- Seg mAP50-95

## 损失函数



## 案例：CRD分割

### 训练

```bash
# Start training from a pretrained *.pt model
yolo detect train data=coco8-seg.yaml model=yolov8n-seg.pt epochs=100 imgsz=640
```



### 验证

### 推理





# YOLOV10

## 损失函数

### one-to-one

### one-to-many



## 安装

- `pip install git+https://github.com/THU-MIG/yolov10.git`   这里指定的ultralytics 版本是 v8.1.34
- `pip install huggingface-hub`   不确定有没有用

## 预训练模型

- `https://github.com/THU-MIG/yolov10/releases/download/v1.1/yolov10{n/s/m/b/l/x}.pt` 
- 



# YOLO



