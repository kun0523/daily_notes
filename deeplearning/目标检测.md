# 目标检测相关

- 课程： 
  - YOLO `https://github.com/datawhalechina/yolo-master/tree/main`

## 数据集标注形式

### VOC
- Pascal VOC
- GT Bounding Box
  - [x1, y1, x2, y2]
  - **左上角坐标，右下角坐标**
- 文件结构
  - `label_list.txt`  类别名称列表  用于说明检测对象的类别
  - `train.txt`  训练数据文件列表   `path/to/image.jpg path/to/annotation.xml`
  - `valid.txt`  验证数据文件列表 
  - `annotations/`  存放标注文件
  - `images/`   存放图片

### COCO
- GT Bounding Box
  - [x1, y1, w, h]
  - **左上角坐标，目标区域宽高**
- 文件结构
  - `label_list.txt`  存放目标类别，一行一个类别
  - `annotations/`  存放训练和测试集标注文件 `json`
    - `train.json`
    - `valid.json`
  - `train/`  存放训练集图片
  - `val/`  存放验证集图片
- 使用工具转换
  - `python x2coco.py --dataset_type labelme --json_input_dir E:\DataSets\dents_det\cut_patches\with_dent --image_input_dir E:\DataSets\dents_det\cut_patches\with_dent --output_dir E:\DataSets\dents_det\cut_patches\coco_tmp --train_proportion 0.8 --val_proportion 0.2 --test_proportion 0.0`
- 使用工具验证标注结果（画出每一个框的位置和类别）

### YOLO

## 评估指标

### Confusion Matrix

- ![confusion-matrix](../image_resources/confusion-matrix.png)
- 目标检测中，`TP、FP` 的条件：1. 类别置信度满足，2. IoU大于阈值；

### ROC曲线 与 AUC

- 横轴是`` 纵轴是``
- `AUC` 是 ROC曲线与X轴围成的面积


### PR曲线 与 AP

- 横轴是 `Recall` 纵轴是 `Precision`，随着置信度阈值从高到低绘制；
- `AP` 是 `PR曲线` 的面积
- `AP` 计算方法：
  - 11点插值（VOC计算方法2008）:`IoU>0.5` `Recall`从 `0~1`插11个点`0, 0.1, 0.2, ...., 1`每个点**取右边的最大值**
  - (VOC计算方法2010后) ：`IoU>0.5` 考虑全部的点
  - ![AP-VOC](../image_resources/AP-VOC.png)
  - (COCO) `AP@[0.5:0.05:0.95]` IoU 从0.5到0.95间隔0.05 分别计算10个平均AP
  - ![AP-COCO](../image_resources/AP-COCO.png)

### mAP

- `mAP` 是指各个类别的平均 `AP`；
- `AP`  是指 `Precision-Recall` 曲线与 X轴 围成的面积；
- `P-R Curve` 纵轴为 `Precision` 横轴为 `Recall`, 置信度阈值从高到低逐渐变化时，对应的 `P-R` 值画出的；
  
- <img src="../image_resources/precision-recall-obj-det.png" alt="Precision-Recall" style="zoom:80%"> 
  
- <img src="../image_resources/PR-curve.png" alt="PR-Curve" style="zoom:80%"> 
- 

### mAP50

- Mean Average Precision at 50% overlap
- 当det bbox 与 gt bbox IOU>50% 时，所有类别的平均Precision
- 先计算各个类别的AP，然后计算所有类别的平均AP
- AP代表Precision-Recall 曲线与x轴围成的面积
- 数值越趋近于1越好；

### mAP50-95

- Mean Average Precision from 50% to 95% overlap
- 相比 mAP50, mAP50-95 考虑IoU 从50%到95%(每隔5%计算一次)，可以更好的评估模型的稳健程度；

- 在评测指标方面，目标检测常用的评测指标有 mAP@0.5 和 mAP@[0.5:0.95]，mAP@0.5 是指目标检测器在交并比(IOU)阈值为 0.5 的条件下在数据集各个类别上的平均精度，mAP@[0.5:0.95] 是指目标检测器在 IOU=0.5 至 IOU=0.95 以 5% 为步长得到的不同阈值为条件而得到的平均精度，mAP@[0.5:0.95] 通常更加侧重反映的是目标检测器的位置检测精度，而在该任务的场景中，我们更加关注目标的检出准确性，需要保证能够有效的防止出现漏检或者误检的情况，因此我们**采用 mAP@0.5 作为检测器的精度评测指标**。


## YOLO 系列

### YOLOv1

- 2015/06
- 提出直接回归预测检测框的算法
- 基于 `Anchor Box` 进行微调预测
- 输出结果 `S*S*(B*5[x1,y1,w1,h1,conf1] + C[classes])` 分为S个格子，每个格子有B个 `Anchor Box`
- ![yolov1-structure](../image_resources/yolov1-structure.png)
- 损失函数：
  - ![yolov1-lossfunc](../image_resources/yolov1-lossfunc.png)
  - 位置误差
  - 含有对象的置信度误差
  - 不含对象的置信度误差
  - 分类误差
- 存在的问题：
  - 小物体易漏检
  - Anchor 长宽比太单一
  - 重叠物体难检测

### YOLOv2

- 仅更新一些技巧
  - `Batch Normalization` 使得更易于收敛，精度更高；
  - 使用更大的分辨率图进行训练；
  - 分格子从（7x7）--> (13x13)，`Recall`提升很多；
  - 没有FC层，省参数
  - 使用聚类方法提取 `Anchor` 的尺寸，并且使用**5个不同**的`Anchor`
- 改用小卷积核的好处
  - 降低参数量
  - 可以使用更多的BN，提升效果

### YOLOv3

- 更新点
  - 改进网络结构 DarkNet，使得更适合小目标检测
  - ![yolov3-backbone](../image_resources/yolov3-backbone.png)
  - `Anchor Box` 更丰富 一共9种 (3种scale[13x13 26x26 52x52] 3种规格)
  - ![multi-scale](../image_resources/yolov3-multiscale.png)
  - ![multi-scale-merge](../image_resources/yolov3-multiscale-merge.png)
  - `SoftMax` 改进，预测多标签任务，改变为多个二分类；

### YOLOv4

- 改进点
  - 数据增强
  - 网络设计
    - `CSPNet [Cross Stage Partial Network]`
      - ![cspnet](../image_resources/cspnet.png)
      - 速度可以提升很多，但精度几乎不下降；
    - `SAM [Spatial Attention Module]`
      - ![CBAM-通道和空间注意力](../image_resources/cbam-yolov4-sam.png)
    - `PAN [Path Aggregation Network]`
      - ![path aggregation network](../image_resources/pan.png)
- 损失函数改进
  - Iou Loss： `1-IoU`
  - GIoU：
    - ![GIoU](../image_resources/giou.png)
  - DIoU:
    - ![DIoU](../image_resources/diou.png)
  - CIoU(**Yolov4中使用的**):
    - 同时考虑了 重叠面积 中心距离 长宽比
    - ![CIoU](../image_resources/ciou.png)
- 激活函数 `Mish`
  - ![yolov4-mish](../image_resources/activate_mish.png)
  
### YOLOv5

### YOLOv7
- 统一了卷积核大小 --> 3x3
- 多个卷积核合并
- 
- 加速操作
  - 卷积与BN操作合并
  - 1x1 改为 3x3（英伟达将3x3优化的非常快）
  - ![yolov7-pnsamples](../image_resources/yolov7-pnsamples.png)

### YOLOv11

- C3K2
- Head 引入深度可分离卷积

#### ultralytics 完整训练过程：
- 图像前处理
- 损失函数计算


### YOLOV12


## 模型计算量

- 深度学习模型的计算量通常指模型在一次推理（inference）或者一个训练迭代（training iteration）中所需进行的总浮点运算次数（FLOPs，Floating Point Operations）。
- 假设有一个深度学习模型在一次前向传播中需要进行 5 亿次浮点运算（500 MFlops）。如果运行环境的计算能力是每秒 2 GFlops，那么这个环境大约能够在 500/2000=0.25500/2000=0.25 秒内完成一次前向传播。反之，如果训练一个模型每批次需要执行 10 GFlops 的运算量，使用一个每秒 5 GFlops 的计算设备，大约需要 2 秒完成一批次的训练。
  1. **GFlops（GigaFlops）**:
     - 1 GFlop 表示每秒执行十亿次（109109）浮点运算。
     - GFlops 是 "Giga Floating Point Operations per Second" 的缩写。
  2. **TFlops（TeraFlops）**:
     - 1 TFlop 表示每秒执行一万亿次（10121012）浮点运算。
     - TFlops 是 "Tera Floating Point Operations per Second" 的缩写。

### 计算实例

- 如果一块 GPU 有 5 TFlops 的计算能力，那么它在一秒钟内可以执行 5×10125×1012 次浮点运算。
- 如果一个神经网络模型在训练过程中需要每秒进行 500 GFlops 的运算量，那么如此 GPU 或系统需要至少具备 0.5 TFlops 的计算能力来满足这个需求。

### 示例代码

- 如果你想在程序中估计或者计算设备的 Flops，可以使用一些基准测试或者分析工具。例如，在深度学习框架中如 PyTorch 或 TensorFlow 可以使用一些内建的工具来测量模型的浮点运算量。

    以 PyTorch 为例：

    ```python
    import torch
    import torch.nn as nn
    
    model = nn.Sequential(
        nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),
        nn.ReLU(),
        nn.MaxPool2d(kernel_size=2, stride=2),
        nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),
        nn.ReLU(),
        nn.MaxPool2d(kernel_size=2, stride=2),
    )
    
    # Example input of the model
    input = torch.randn(1, 3, 224, 224)
    
    # Use torchinfo summary to get the number of FLOPs
    from torchinfo import summary
    summary(model, input_size=(1, 3, 224, 224))
    ```

    总之，GFlops 和 TFlops 提供了计算设备性能和深度学习计算需求的一个度量标准，通过理解这些单位的具体含义和应用情景，可以更好地设计、选择和使用计算设备。
- 将第三方库安装在指定路径下，并在脚本中引入
  
  - `pip install beautifulsoup4 -t /path/to/third_party_lib`
  - `import sys   sys.path.append('/path/to/third_party_lib')` 
- 学习率
  - 探索学习率，最小为默认学习率的 1/10, 最大为默认学习率的 5 倍
  - 成倍增加显卡数量时，学习率也应该线性增加，例如，单卡学习率 0.1  换成4卡应改为 0.4；换成8卡应改为0.8
  
- lr/pg0

- lr/pg1

- lr/pg2

## 超参数配置

## 恢复训练 resume

- `yolo train resume model=path/to/last.pt` 
  - 从 last.pt 文件中加载 之前的训练参数、优化器状态、学习率规划
  - 日志文件还记录在之前的日志文件中

## 数据增强

- 无数据增强时，仅将图片resize到指定尺寸（保持比例，边缘padding的方式）
- 公用
  - mixup：将两张图片叠加到一起；
  - mosaic：多张图片拼接
  - scale：缩放， +-0~1，产品比例
  - degrees：旋转角度，+- 角度值；<img src="..\image_resources\train_batch0.jpg" alt="train_batch0" style="zoom: 20%;" />
  - translate：平移图片，+- 0~1，移动产品大小的比例；
  - shear：<img src="..\image_resources\train_batch2.jpg" alt="train_batch2" style="zoom:20%;" />



- 检测
  - 
- 分割
  - copy_paste
- 分类
  - auto_augment
  - erasing
  - crop_fraction


## 损失函数

- 只有一个类别时，样本不均是指，目标类别和背景类别比例差别大

### box_loss

- Smooth L1 loss
- Mean Squared Error
- 标注坐标 `(tlx, tly, brx, bry)` 
- 在YOLO框架下，box_loss 是通过 CIOU(Complete Intersection over Union) 计算的，不仅考虑IOU，还考虑两个框的长宽比是否相近

### cls_loss

- cross entropy loss
- binary cross entropy loss
- 预测类别与真实类别的差异

### dfl_loss

- Distribution Focal Loss  分布式 Focal Loss
- yolov8 中 使用 dfl 促进 bbox 框的准确性？？

### focal_loss

- 用于解决正负样本不均衡问题

- 有RetinaNet 在 2017年提出的；

- 算法思想：降低简单样本的训练权重，提高困难样本的训练权重

- $FL(p_t)=−(1−p_t)^γ⋅log(p_t)$

  where:

  - $p_t$ 预测为正确类别的概率

  - $γ$  聚焦参数，通常取值（0~5）控制多大程度上降低简单的样本的权重

    
  
- 

  


## 案例：压痕检测
### 数据标注

### 训练与验证
- 训练评价指标无法提升时，检讨数据集标注情况，本案例中遇到的问题是，加入新的压痕形式后精度没有提升，增加很多随机裁剪后，精度有显著提升，说明新形式压痕的数据在训练集中出现太少，没有学到有效特征
- 争取要让不同形式的样本数据均衡，才有利于模型训练
### 部署
### 效果

## 案例：PCB区域检测

### 数据标注
- `class_id center_x center_y width height`
- bbox 中心坐标 宽高 除以图片的宽高进行归一化

#### 数据标注错误
- 如果将数据错误标注成 `class_id tlx tly brx brh`  在使用ultralytics框架时，train batch可视化会有显著差异

### 训练

- 第一次训练：还没有生成config文件
  - `yolo train model=yolov8n.pt data=path/to/data_config.yaml` 
- 生成config.yaml后就可以自行修改配置文件
  - `yolo cfg=path/to/train_config.yaml`
- 导出onnx模型
  - `yolo export opset=12 format=onnx model=path/to/best.pt` 

### 验证

### 推理

- `yolo predict model=path_to_model.pt source=path_to_images_dir` 

#### 使用python YOLO 推理

##### 图像前处理

- LetterBox  Resize image and padding for detection
  - 算法流程：
  - 1. 最小缩放比：`r=min(new_shape[0]/old_shape[0], new_shape[1]/old_shape[1])` 
    2. 计算padding：

##### 结果后处理

- 模型返回值  `batch_size * (4 coordinates + num_cls) * 8400`
- **4 coordinates: center_x, center_y, width, height**  



#### 使用openvino推理

- **图片前处理**
- **模型返回结果的后处理**

- 模型返回结果： `1*7*8400` 
  
- 7: 前四个表示坐标  x, y, w, h; 后面几位代表各个类别的置信度
  
- cpp

- ```cpp
  #include <iostream>
  #include <opencv2/opencv.hpp>
  #include <openvino/openvino.hpp>
  #include <algorithm>
  
  using std::cout;
  using std::endl;
  using std::string;
  using std::vector;
  
  int main(){
      string onnx_pth = R"(D:\share_dir\pd_mix\workdir\det_pcb\train\weights\best.onnx)";
      ov::Core core;
      auto compiled_model = core.compile_model(onnx_pth, "CPU");
      ov::InferRequest infer_request = compiled_model.create_infer_request();
  
      string image_pth = R"(E:\DataSets\pd_mix\test_small_pic\20240407_00001_P51-L_A741427074_13_17_C1_FK_A2FK2S42HOICD123_A2FK2S4338MCD090.jpg)";
      cv::Mat img = cv::imread(image_pth);
      int org_img_h = img.rows, org_img_w = img.cols;
      cout <<"original image size: " << img.size << endl;
      cv::Mat blob_img = cv::dnn::blobFromImage(img, 1.0/255.0, cv::Size(640, 640), cv::Scalar(0.0,0.0,0.0), true, false, CV_32F);
      cout << "blob size: " << blob_img.size << endl;
  
      auto input_port = compiled_model.input();
      ov::Tensor input_tensor(input_port.get_element_type(), input_port.get_shape(), blob_img.ptr());
      infer_request.set_input_tensor(input_tensor);
      infer_request.infer();
  
      // 1*7*8400  cx, cy, w, h, c1, c2, c3...
      ov::Shape output_tensor_shape = compiled_model.output().get_shape();
      size_t batch=output_tensor_shape[0], preds=output_tensor_shape[1], bbox_num=output_tensor_shape[2];
      cout << output_tensor_shape << endl;
      auto output_tensor = infer_request.get_output_tensor();
      const float* output_buff = output_tensor.data<const float>();
  
      cv::Mat m = cv::Mat(cv::Size(bbox_num, preds), CV_32F, const_cast<float*>(output_buff));
      m = m.t();
      cout <<"Transpose: " << m.size << endl;
  
      float w_factor{float(org_img_w)/640}, h_factor{float(org_img_h)/640};
      float score_threshold = 0.5;
      float iou_threshold = 0.5;
      vector<cv::Rect> boxes;
      vector<float> scores;
      vector<int> indices;
      for(int row=0; row<m.size[0]; ++row){
          float* ptr = m.ptr<float>(row);
          float cx = ptr[0], cy = ptr[1], w=ptr[2], h=ptr[3];
          vector<float> cls_conf = vector<float>(ptr+4, ptr+7);
          cv::Point maxP;
          double maxV;
          cv::minMaxLoc(cls_conf, 0, &maxV, 0, &maxP);
          boxes.push_back(cv::Rect2d(cx-w/2, cy-h/2, w, h));
          scores.push_back(static_cast<float>(maxV));
      }
  
  
      cv::dnn::NMSBoxes(boxes, scores, score_threshold, iou_threshold, indices);
      cout << "pred bbox num:  " << indices.size() << endl;
  
      for(auto it=indices.begin(); it!=indices.end(); ++it){
          cv::Rect2d tmp = boxes[*it];
          tmp.x *= w_factor;
          tmp.y *= h_factor;
          tmp.width *= w_factor;
          tmp.height *= h_factor;
          cv::rectangle(img, tmp, cv::Scalar(0,0,255), 10);
      }
      if(cv::imwrite("test.jpg", img))
          cout << "Save Success." << endl;
  }
  ```

- 



### 模型效果 GradCAM






# YOLOV10

## 损失函数

### one-to-one

### one-to-many



## 安装

- `pip install git+https://github.com/THU-MIG/yolov10.git`   这里指定的ultralytics 版本是 v8.1.34
- `pip install huggingface-hub`   不确定有没有用

## 预训练模型

- `https://github.com/THU-MIG/yolov10/releases/download/v1.1/yolov10{n/s/m/b/l/x}.pt` 
- 